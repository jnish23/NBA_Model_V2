{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import selenium\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.etl import *\n",
    "from src.data.update_data import *\n",
    "\n",
    "import optuna\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Jordan Nishimura/NBA_Model_v1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.home().joinpath('NBA_Model_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Update Data\n",
    "2) Preprocess and reload into SQL DB\n",
    "3) Pull Updated Preprocessed Data from SQL DB\n",
    "4) Train Model on fully updated data\n",
    "5) Pull Days Matchups\n",
    "6) Predict on Days Matchups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 1/1 [00:12<00:00, 12.65s/it]\n"
     ]
    }
   ],
   "source": [
    "db_path = Path.home() / 'NBA_Model_v1' / 'data' / 'nba.db'\n",
    "season = 2021\n",
    "update_all_data(db_path=db_path, season=season)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Reload into SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw team boxscore data from sql database...\n",
      "Loading betting data from sql database...\n",
      "Cleaning Data...\n",
      "Merging Boxscore and Betting Data...\n",
      "Aggregating over last 5, 10, and 20 game windows\n",
      "adding rest days\n",
      "creating matchups between Home and Away team aggregated stats\n",
      "Resorting by date\n",
      "dropping nulls\n",
      "loading table back into sql db as team_stats_ewa_matchup\n"
     ]
    }
   ],
   "source": [
    "%run ..\\\\src\\\\etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_to_string(x):\n",
    "    return str(x) + '-' + str(x+1)[-2:]\n",
    "\n",
    "def get_training_data_all(target, con):   \n",
    "\n",
    "    df = pd.read_sql('SELECT * FROM team_stats_ewa_matchup', con=con)\n",
    "    df = df.drop(columns=['index'])\n",
    "\n",
    "    df = df.sort_values('GAME_DATE')\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    columns_to_drop = ['SEASON', 'HOME_TEAM_ABBREVIATION', 'GAME_DATE', 'GAME_ID', 'MATCHUP',\n",
    "                        'HOME_HOME_GAME', 'HOME_TEAM_SCORE', 'HOME_ML', 'HOME_SPREAD',\n",
    "                        'HOME_ATS_DIFF', 'HOME_TEAM_COVERED', 'HOME_POINT_DIFF',\n",
    "                        'HOME_WL', 'AWAY_ML', 'AWAY_TEAM_SCORE',\n",
    "                        'HOME_PTS_L5', 'HOME_PTS_L10', 'HOME_PTS_L20',\n",
    "                        'HOME_PLUS_MINUS_L5', 'HOME_PLUS_MINUS_L10', 'HOME_PLUS_MINUS_L20',\n",
    "                        'HOME_NET_RATING_L5', 'HOME_NET_RATING_L10', 'HOME_NET_RATING_L20',\n",
    "                        'HOME_POSS_L5', 'HOME_POSS_L10', 'HOME_POSS_L20',\n",
    "                        'HOME_PTS_opp_L5', 'HOME_PTS_opp_L10', 'HOME_PTS_opp_L20',\n",
    "                        'HOME_PLUS_MINUS_opp_L5', 'HOME_PLUS_MINUS_opp_L10', 'HOME_PLUS_MINUS_opp_L20',\n",
    "                        'HOME_NET_RATING_opp_L5', 'HOME_NET_RATING_opp_L10', 'HOME_NET_RATING_opp_L20',\n",
    "                        'HOME_POSS_opp_L5', 'HOME_POSS_opp_L10', 'HOME_POSS_opp_L20',\n",
    "                        'HOME_REB_L5', 'HOME_REB_L10', 'HOME_REB_L20',  \n",
    "                        'HOME_REB_opp_L5', 'HOME_REB_opp_L10', 'HOME_REB_opp_L20',       \n",
    "                        'AWAY_PTS_L5', 'AWAY_PTS_L10', 'AWAY_PTS_L20',\n",
    "                        'AWAY_PLUS_MINUS_L5', 'AWAY_PLUS_MINUS_L10', 'AWAY_PLUS_MINUS_L20',\n",
    "                        'AWAY_NET_RATING_L5', 'AWAY_NET_RATING_L10', 'AWAY_NET_RATING_L20',\n",
    "                        'AWAY_POSS_L5', 'AWAY_POSS_L10', 'AWAY_POSS_L20',\n",
    "                        'AWAY_PTS_opp_L5', 'AWAY_PTS_opp_L10', 'AWAY_PTS_opp_L20',\n",
    "                        'AWAY_PLUS_MINUS_opp_L5', 'AWAY_PLUS_MINUS_opp_L10', 'AWAY_PLUS_MINUS_opp_L20',\n",
    "                        'AWAY_NET_RATING_opp_L5', 'AWAY_NET_RATING_opp_L10', 'AWAY_NET_RATING_opp_L20',\n",
    "                        'AWAY_POSS_opp_L5', 'AWAY_POSS_opp_L10', 'AWAY_POSS_opp_L20',\n",
    "                        'AWAY_REB_L5', 'AWAY_REB_L10', 'AWAY_REB_L20',\n",
    "                        'AWAY_REB_opp_L5', 'AWAY_REB_opp_L10', 'AWAY_REB_opp_L20']\n",
    "\n",
    "    X_train = df.drop(columns=columns_to_drop)\n",
    "    y_train = df[target]\n",
    "\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['HOME_TEAM_SCORE', 'AWAY_TEAM_SCORE']\n",
    "db_filepath = Path.home().joinpath('NBA_model_v1', 'data', 'nba.db')\n",
    "connection = sqlite3.connect(db_filepath)\n",
    "\n",
    "X_train, y_train = get_training_data_all(target = target, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10955, 566)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.7318848324101718, 'learning_rate': 0.10103401117148772, 'max_depth': 61, 'min_child_weight': 0.8780493419526695, 'num_leaves': 10, 'reg_alpha': 0.6129570091832104, 'reg_lambda': 8.769459451498605, 'subsample': 0.8136328018522077}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=LGBMRegressor(colsample_bytree=0.7318848324101718,\n",
       "                                             learning_rate=0.10103401117148772,\n",
       "                                             max_depth=61,\n",
       "                                             min_child_weight=0.8780493419526695,\n",
       "                                             num_leaves=10,\n",
       "                                             reg_alpha=0.6129570091832104,\n",
       "                                             reg_lambda=8.769459451498605,\n",
       "                                             subsample=0.8136328018522077))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputRegressor</label><div class=\"sk-toggleable__content\"><pre>MultiOutputRegressor(estimator=LGBMRegressor(colsample_bytree=0.7318848324101718,\n",
       "                                             learning_rate=0.10103401117148772,\n",
       "                                             max_depth=61,\n",
       "                                             min_child_weight=0.8780493419526695,\n",
       "                                             num_leaves=10,\n",
       "                                             reg_alpha=0.6129570091832104,\n",
       "                                             reg_lambda=8.769459451498605,\n",
       "                                             subsample=0.8136328018522077))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(colsample_bytree=0.7318848324101718,\n",
       "              learning_rate=0.10103401117148772, max_depth=61,\n",
       "              min_child_weight=0.8780493419526695, num_leaves=10,\n",
       "              reg_alpha=0.6129570091832104, reg_lambda=8.769459451498605,\n",
       "              subsample=0.8136328018522077)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(colsample_bytree=0.7318848324101718,\n",
       "              learning_rate=0.10103401117148772, max_depth=61,\n",
       "              min_child_weight=0.8780493419526695, num_leaves=10,\n",
       "              reg_alpha=0.6129570091832104, reg_lambda=8.769459451498605,\n",
       "              subsample=0.8136328018522077)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputRegressor(estimator=LGBMRegressor(colsample_bytree=0.7318848324101718,\n",
       "                                             learning_rate=0.10103401117148772,\n",
       "                                             max_depth=61,\n",
       "                                             min_child_weight=0.8780493419526695,\n",
       "                                             num_leaves=10,\n",
       "                                             reg_alpha=0.6129570091832104,\n",
       "                                             reg_lambda=8.769459451498605,\n",
       "                                             subsample=0.8136328018522077))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load study with best hyperparameters\n",
    "study_name = str(Path.home().joinpath('NBA_model_v1', 'models', 'hyperparameter_tuning', 'LGBMRegressor'))    \n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "\n",
    "study = optuna.load_study(study_name = study_name, storage = storage_name)\n",
    "\n",
    "params = study.best_params\n",
    "print(params)\n",
    "\n",
    "# instantiate model with hyperparameters\n",
    "lgbr_model = MultiOutputRegressor(lgb.LGBMRegressor(**params))\n",
    "\n",
    "## train model on full data\n",
    "\n",
    "lgbr_model.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.04008569825513905, 'epsilon': 5.2966062017695155, 'l1_ratio': 0.9498835228685188, 'loss': 'huber'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                               (&#x27;sgd&#x27;,\n",
       "                                                SGDRegressor(alpha=0.04008569825513905,\n",
       "                                                             epsilon=5.2966062017695155,\n",
       "                                                             l1_ratio=0.9498835228685188,\n",
       "                                                             loss=&#x27;huber&#x27;,\n",
       "                                                             max_iter=10000))]))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputRegressor</label><div class=\"sk-toggleable__content\"><pre>MultiOutputRegressor(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                               (&#x27;sgd&#x27;,\n",
       "                                                SGDRegressor(alpha=0.04008569825513905,\n",
       "                                                             epsilon=5.2966062017695155,\n",
       "                                                             l1_ratio=0.9498835228685188,\n",
       "                                                             loss=&#x27;huber&#x27;,\n",
       "                                                             max_iter=10000))]))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;sgd&#x27;,\n",
       "                 SGDRegressor(alpha=0.04008569825513905,\n",
       "                              epsilon=5.2966062017695155,\n",
       "                              l1_ratio=0.9498835228685188, loss=&#x27;huber&#x27;,\n",
       "                              max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(alpha=0.04008569825513905, epsilon=5.2966062017695155,\n",
       "             l1_ratio=0.9498835228685188, loss=&#x27;huber&#x27;, max_iter=10000)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputRegressor(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                               ('sgd',\n",
       "                                                SGDRegressor(alpha=0.04008569825513905,\n",
       "                                                             epsilon=5.2966062017695155,\n",
       "                                                             l1_ratio=0.9498835228685188,\n",
       "                                                             loss='huber',\n",
       "                                                             max_iter=10000))]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load study with best hyperparameters\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "study_name = str(Path.home().joinpath('NBA_model_v1', 'models', 'hyperparameter_tuning', 'SGDRegressor_ScorePredictor'))    \n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "\n",
    "study = optuna.load_study(study_name = study_name, storage = storage_name)\n",
    "\n",
    "params = study.best_params\n",
    "print(params)\n",
    "\n",
    "# instantiate model with hyperparameters\n",
    "sgd_model = MultiOutputRegressor(Pipeline([('scaler', StandardScaler()),\n",
    "                                           ('sgd', SGDRegressor(**params, max_iter=10000))\n",
    "                                           ]\n",
    "                                          )\n",
    "                                 )\n",
    "\n",
    "## train model on full data\n",
    "\n",
    "sgd_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_TEAM = 'SAS'\n",
    "AWAY_TEAM = 'LAL'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "\n",
    "def load_team_data(conn, start_season, end_season):\n",
    "    \"\"\"Loads basic, advanced, and scoring boxscores \n",
    "    from sqlite db and merges them into one dataframe\"\"\"\n",
    "    \n",
    "\n",
    "    basic = pd.read_sql(\"SELECT * FROM team_basic_boxscores\", conn)\n",
    "    adv = pd.read_sql(\"SELECT * FROM team_advanced_boxscores\", conn)\n",
    "    scoring = pd.read_sql(\"SELECT * FROM team_scoring_boxscores\", conn)\n",
    "    tracking = pd.read_sql(\"SELECT * FROM team_tracking_boxscores\", conn)\n",
    "\n",
    "    basic = basic.loc[basic['SEASON'].between(start_season, end_season)]\n",
    "    basic[['GAME_ID', 'TEAM_ID']] = basic[['GAME_ID', 'TEAM_ID']].astype(str)\n",
    "    adv[['GAME_ID', 'TEAM_ID']] = adv[['GAME_ID', 'TEAM_ID']].astype(str)\n",
    "    scoring[['GAME_ID', 'TEAM_ID']] = scoring[['GAME_ID', 'TEAM_ID']].astype(str)\n",
    "    tracking[['GAME_ID', 'TEAM_ID']] = tracking[['GAME_ID', 'TEAM_ID']].astype(str)\n",
    "\n",
    "    df = pd.merge(basic, adv, how='left', on=[\n",
    "                    'GAME_ID', 'TEAM_ID'], suffixes=['', '_y'])\n",
    "    df = pd.merge(df, scoring, how='left', on=[\n",
    "                  'GAME_ID', 'TEAM_ID'], suffixes=['', '_y'])\n",
    "    \n",
    "    df = pd.merge(df, tracking, how='left', on=['GAME_ID', 'TEAM_ID'],\n",
    "                  suffixes=['', '_y'])\n",
    "    \n",
    "\n",
    "    df = df.drop(columns=['TEAM_NAME_y', 'TEAM_CITY',\n",
    "                          'TEAM_ABBREVIATION_y',\n",
    "                          'TEAM_CITY_y', 'MIN_y',\n",
    "                          'FG_PCT_y', 'AST_y'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_team_data(df):\n",
    "    \"\"\"This function cleans the team_data\n",
    "    1) Changes W/L to 1/0 \n",
    "    2) Changes franchise abbreviations to their most \n",
    "    recent abbreviation for consistency\n",
    "    3) Converts GAME_DATE to datetime object\n",
    "    4) Creates a binary column 'HOME_GAME'\n",
    "    5) Removes 3 games where advanced stats were not collected\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['WL'] = (df['WL'] == 'W').astype(int)\n",
    "\n",
    "    abbr_mapping = {'NJN': 'BKN',\n",
    "                    'CHH': 'CHA',\n",
    "                    'VAN': 'MEM',\n",
    "                    'NOH': 'NOP',\n",
    "                    'NOK': 'NOP',\n",
    "                    'SEA': 'OKC'}\n",
    "\n",
    "    df['TEAM_ABBREVIATION'] = df['TEAM_ABBREVIATION'].replace(abbr_mapping)\n",
    "    df['MATCHUP'] = df['MATCHUP'].str.replace('NJN', 'BKN')\n",
    "    df['MATCHUP'] = df['MATCHUP'].str.replace('CHH', 'CHA')\n",
    "    df['MATCHUP'] = df['MATCHUP'].str.replace('VAN', 'MEM')\n",
    "    df['MATCHUP'] = df['MATCHUP'].str.replace('NOH', 'NOP')\n",
    "    df['MATCHUP'] = df['MATCHUP'].str.replace('NOK', 'NOP')\n",
    "    df['MATCHUP'] = df['MATCHUP'].str.replace('SEA', 'OKC')\n",
    "\n",
    "    df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])\n",
    "\n",
    "    df['HOME_GAME'] = df['MATCHUP'].str.contains('vs').astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# from src.data.make_team_dataset import prep_for_aggregation\n",
    "\n",
    "def prep_for_aggregation(df):\n",
    "    \"\"\"This function...\n",
    "    1) Removes categories that are percentages,\n",
    "    as we will be averaging them and do not want to average \n",
    "    percentages. \n",
    "    2) Converts shooting percentage stats into raw values\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.drop(columns=['FT_PCT', 'FG_PCT', 'FG3_PCT', 'DREB_PCT',\n",
    "                          'OREB_PCT', 'REB_PCT', 'AST_PCT', 'AST_TOV',\n",
    "                          'AST_RATIO', 'E_TM_TOV_PCT', 'TM_TOV_PCT',\n",
    "                          'EFG_PCT', 'TS_PCT', 'USG_PCT', 'E_USG_PCT',\n",
    "                          'PACE_PER40', 'MIN', 'PIE', 'CFG_PCT', 'UFG_PCT',\n",
    "                          'DFG_PCT', 'E_OFF_RATING', 'E_DEF_RATING', 'E_NET_RATING'])\n",
    "\n",
    "    df['FG2M'] = df['FGM'] - df['FG3M']\n",
    "    df['FG2A'] = df['FGA'] - df['FG3A']\n",
    "    df['PTS_2PT_MR'] = (df['PTS'] * df['PCT_PTS_2PT_MR']).astype('int8')\n",
    "    df['PTS_FB'] = (df['PTS'] * df['PCT_PTS_FB']).astype('int8')\n",
    "    df['PTS_OFF_TOV'] = (df['PTS'] * df['PCT_PTS_OFF_TOV']).astype('int8')\n",
    "    df['PTS_PAINT'] = (df['PTS'] * df['PCT_PTS_PAINT']).astype('int8')\n",
    "    df['AST_2PM'] = (df['FG2M'] * df['PCT_AST_2PM']).astype('int8')\n",
    "    df['AST_3PM'] = (df['FG3M'] * df['PCT_AST_3PM']).astype('int8')\n",
    "    df['UAST_2PM'] = (df['FG2M'] * df['PCT_UAST_2PM']).astype('int8')\n",
    "    df['UAST_3PM'] = (df['FG3M'] * df['PCT_UAST_3PM']).astype('int8')\n",
    "\n",
    "    df['POINT_DIFF'] = df['PLUS_MINUS']\n",
    "    df['RECORD'] = df['WL']\n",
    "    df['TEAM_SCORE'] = df['PTS']\n",
    "    \n",
    "    df = df.drop(columns = ['PCT_FGA_2PT', 'PCT_FGA_3PT', 'PCT_PTS_2PT',\n",
    "                          'PCT_PTS_2PT_MR', 'PCT_PTS_3PT', 'PCT_PTS_FB',\n",
    "                          'PCT_PTS_FT','PCT_PTS_OFF_TOV', 'PCT_PTS_PAINT', \n",
    "                          'PCT_AST_2PM', 'PCT_UAST_2PM','PCT_AST_3PM',\n",
    "                          'PCT_UAST_3PM', 'PCT_AST_FGM', 'PCT_UAST_FGM',\n",
    "                          'E_PACE'])\n",
    "    \n",
    "    ## Reorder Columns\n",
    "    \n",
    "    df = df[['SEASON', 'TEAM_ID', 'TEAM_ABBREVIATION', 'TEAM_NAME', 'GAME_ID',\n",
    "       'GAME_DATE', 'MATCHUP', 'TEAM_SCORE', 'WL', 'POINT_DIFF', 'HOME_GAME', 'RECORD',\n",
    "       'FG2M', 'FG2A', 'FG3M', 'FG3A', 'FTM', 'FTA', 'OREB', 'DREB', 'REB', \n",
    "       'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS',\n",
    "       'PLUS_MINUS', 'OFF_RATING', 'DEF_RATING', 'NET_RATING', 'PACE',\n",
    "       'POSS', 'DIST', 'ORBC', 'DRBC', 'RBC', 'TCHS', 'SAST', 'FTAST', 'PASS',\n",
    "       'CFGM', 'CFGA', 'UFGM', 'UFGA', 'DFGM', 'DFGA', 'PTS_2PT_MR', 'PTS_FB', 'PTS_OFF_TOV', 'PTS_PAINT', 'AST_2PM',\n",
    "       'AST_3PM', 'UAST_2PM', 'UAST_3PM']]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_betting_data(conn):\n",
    "    spreads = pd.read_sql(\"SELECT * FROM spreads\", conn)\n",
    "    moneylines = pd.read_sql(\"SELECT * FROM moneylines\", conn)\n",
    "\n",
    "    return spreads, moneylines\n",
    "\n",
    "\n",
    "def convert_american_to_decimal(x):\n",
    "    return np.where(x>0, (100+x)/100, 1+(100.0/-x))          \n",
    "\n",
    "\n",
    "def clean_moneyline_df(df):\n",
    "    abbr_mapping = {'Boston': 'BOS', 'Portland': 'POR',\n",
    "                    'L.A. Lakers': 'LAL', 'Brooklyn': 'BKN',\n",
    "                    'Cleveland': 'CLE', 'Toronto': 'TOR',\n",
    "                    'Philadelphia': 'PHI', 'Memphis': 'MEM',\n",
    "                    'Minnesota': 'MIN', 'New Orleans': 'NOP',\n",
    "                    'Oklahoma City': 'OKC', 'Dallas': 'DAL',\n",
    "                    'San Antonio': 'SAS', 'Denver': 'DEN',\n",
    "                    'Golden State': 'GSW', 'L.A. Clippers': 'LAC',\n",
    "                    'Orlando': 'ORL', 'Utah': 'UTA',\n",
    "                    'Charlotte': 'CHA', 'Detroit': 'DET',\n",
    "                    'Miami': 'MIA', 'Phoenix': 'PHX',\n",
    "                    'Atlanta': 'ATL', 'New York': 'NYK',\n",
    "                    'Indiana': 'IND', 'Chicago': 'CHI',\n",
    "                    'Houston': 'HOU', 'Milwaukee': 'MIL',\n",
    "                    'Sacramento': 'SAC', 'Washington': 'WAS'}\n",
    "\n",
    "    df['HOME_TEAM'] = df['HOME_TEAM'].replace(abbr_mapping)\n",
    "    df['AWAY_TEAM'] = df['AWAY_TEAM'].replace(abbr_mapping)\n",
    "\n",
    "    away_mls = df['AWAY_ML'].str.split(\",\", expand=True)\n",
    "    home_mls = df['HOME_ML'].str.split(\",\", expand=True)\n",
    "\n",
    "    away_mls = away_mls.replace('-', np.nan).replace('', np.nan)\n",
    "    away_mls = away_mls.fillna(value=np.nan)\n",
    "    away_mls = away_mls.astype(float)\n",
    "\n",
    "    home_mls = home_mls.replace('-', np.nan).replace('', np.nan)\n",
    "    home_mls = home_mls.fillna(value=np.nan)\n",
    "    home_mls = home_mls.astype(float)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "        highest_away_ml = away_mls.apply(lambda row: np.nanmax(\n",
    "            abs(row)) if np.nanmax(row) > 0 else -np.nanmax(abs(row)), axis=1)\n",
    "        highest_away_ml = convert_american_to_decimal(highest_away_ml)\n",
    "        highest_away_ml = pd.DataFrame(\n",
    "            highest_away_ml, columns=['HIGHEST_AWAY_ML'])\n",
    "\n",
    "        highest_home_ml = home_mls.apply(lambda row: np.nanmax(\n",
    "            abs(row)) if np.nanmax(row) > 0 else -np.nanmax(abs(row)), axis=1)\n",
    "        highest_home_ml = convert_american_to_decimal(highest_home_ml)\n",
    "        highest_home_ml = pd.DataFrame(\n",
    "            highest_home_ml, columns=['HIGHEST_HOME_ML'])\n",
    "\n",
    "    moneylines = pd.concat(\n",
    "        [df.iloc[:, :4], highest_home_ml, highest_away_ml], axis=1)\n",
    "    \n",
    "    moneylines['GM_DATE'] = pd.to_datetime(moneylines['GM_DATE'])\n",
    "\n",
    "    return moneylines\n",
    "\n",
    "def clean_spreads_df(df):\n",
    "    abbr_mapping = {'Boston': 'BOS', 'Portland': 'POR',\n",
    "                    'L.A. Lakers': 'LAL', 'Brooklyn': 'BKN',\n",
    "                    'Cleveland': 'CLE', 'Toronto': 'TOR',\n",
    "                    'Philadelphia': 'PHI', 'Memphis': 'MEM',\n",
    "                    'Minnesota': 'MIN', 'New Orleans': 'NOP',\n",
    "                    'Oklahoma City': 'OKC', 'Dallas': 'DAL',\n",
    "                    'San Antonio': 'SAS', 'Denver': 'DEN',\n",
    "                    'Golden State': 'GSW', 'L.A. Clippers': 'LAC',\n",
    "                    'Orlando': 'ORL', 'Utah': 'UTA',\n",
    "                    'Charlotte': 'CHA', 'Detroit': 'DET',\n",
    "                    'Miami': 'MIA', 'Phoenix': 'PHX',\n",
    "                    'Atlanta': 'ATL', 'New York': 'NYK',\n",
    "                    'Indiana': 'IND', 'Chicago': 'CHI',\n",
    "                    'Houston': 'HOU', 'Milwaukee': 'MIL',\n",
    "                    'Sacramento': 'SAC', 'Washington': 'WAS'}\n",
    "\n",
    "    df['HOME_TEAM'] = df['HOME_TEAM'].replace(abbr_mapping)\n",
    "    df['AWAY_TEAM'] = df['AWAY_TEAM'].replace(abbr_mapping)\n",
    "\n",
    "    away_spreads = df['AWAY_SPREAD'].str.split(\",\", expand=True)\n",
    "    home_spreads = df['HOME_SPREAD'].str.split(\",\", expand=True)\n",
    "\n",
    "    for col in away_spreads.columns:\n",
    "        away_spreads[col] = away_spreads[col].str[:-4]\n",
    "        away_spreads[col] = away_spreads[col].str.replace('½', '.5')\n",
    "        away_spreads[col] = away_spreads[col].str.replace('PK', '0')\n",
    "\n",
    "        away_spreads[col] = away_spreads[col].astype(str).apply(\n",
    "            lambda x: x if x == '' else (x[:-1] if x[-1] == '-' else x))\n",
    "\n",
    "    away_spreads = away_spreads.replace('-', np.nan)\n",
    "    away_spreads = away_spreads.replace('', np.nan)\n",
    "    away_spreads = away_spreads.replace('None', np.nan)\n",
    "    away_spreads = away_spreads.fillna(value=np.nan)\n",
    "\n",
    "    away_spreads = away_spreads.astype(float)\n",
    "\n",
    "    for col in home_spreads.columns:\n",
    "        home_spreads[col] = home_spreads[col].str[:-4]\n",
    "        home_spreads[col] = home_spreads[col].str.replace('½', '.5')\n",
    "        home_spreads[col] = home_spreads[col].str.replace('PK', '0')\n",
    "\n",
    "        home_spreads[col] = home_spreads[col].astype(str).apply(\n",
    "            lambda x: x if x == '' else (x[:-1] if x[-1] == '-' else x))\n",
    "\n",
    "    home_spreads = home_spreads.replace('-', np.nan).replace('', np.nan).replace('None', np.nan)\n",
    "    home_spreads = home_spreads.fillna(value=np.nan)\n",
    "\n",
    "    home_spreads = home_spreads.astype(float)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        highest_away_spread = away_spreads.apply(\n",
    "            lambda row: -np.nanmax(abs(row)) if np.nanmax(row) < 0 else np.nanmax(abs(row)), axis=1)\n",
    "        highest_away_spread = pd.DataFrame(\n",
    "            highest_away_spread, columns=['HIGHEST_AWAY_SPREAD'])\n",
    "\n",
    "        highest_home_spread = home_spreads.apply(\n",
    "            lambda row: -np.nanmax(abs(row)) if np.nanmax(row) < 0 else np.nanmax(abs(row)), axis=1)\n",
    "        highest_home_spread = pd.DataFrame(\n",
    "            highest_home_spread, columns=['HIGHEST_HOME_SPREAD'])\n",
    "\n",
    "    spreads = pd.concat(\n",
    "        [df.iloc[:, :4], highest_home_spread, highest_away_spread], axis=1)\n",
    "    spreads['GM_DATE'] = pd.to_datetime(spreads['GM_DATE'])\n",
    "\n",
    "    return spreads\n",
    "\n",
    "\n",
    "def merge_betting_and_boxscore_data(clean_spreads, clean_mls, clean_boxscores):\n",
    "    clean_boxscores['HOME_TEAM'] = clean_boxscores['MATCHUP'].apply(\n",
    "        lambda x: x[:3] if 'vs' in x else x[-3:])\n",
    "    clean_boxscores['AWAY_TEAM'] = clean_boxscores['MATCHUP'].apply(\n",
    "        lambda x: x[:3] if '@' in x else x[-3:])\n",
    "\n",
    "    temp = pd.merge(clean_mls, clean_spreads, on=[\n",
    "                    'SEASON', 'GM_DATE', 'HOME_TEAM', 'AWAY_TEAM'])\n",
    "\n",
    "    merged_df = pd.merge(clean_boxscores, temp, how='left', \n",
    "                         left_on=['SEASON', 'HOME_TEAM', 'AWAY_TEAM', 'GAME_DATE'],\n",
    "                         right_on=['SEASON', 'HOME_TEAM', 'AWAY_TEAM', 'GM_DATE'])\n",
    "\n",
    "    merged_df['ML'] = merged_df.apply(lambda row: row['HIGHEST_HOME_ML'] if row['HOME_GAME'] == 1\n",
    "                                      else row['HIGHEST_AWAY_ML'], axis=1)\n",
    "\n",
    "    merged_df['SPREAD'] = merged_df.apply(lambda row: row['HIGHEST_HOME_SPREAD'] if row['HOME_GAME'] == 1\n",
    "                                          else -row['HIGHEST_HOME_SPREAD'], axis=1)\n",
    "\n",
    "    merged_df = merged_df.drop(columns=['HOME_TEAM', 'AWAY_TEAM', 'GM_DATE',\n",
    "                                        'HIGHEST_HOME_ML', 'HIGHEST_AWAY_ML',\n",
    "                                        'HIGHEST_HOME_SPREAD', 'HIGHEST_AWAY_SPREAD'])\n",
    "\n",
    "    merged_df['ATS_DIFF'] = merged_df['POINT_DIFF'] + merged_df['SPREAD']\n",
    "\n",
    "    merged_df['TEAM_COVERED'] = (merged_df['ATS_DIFF'] > 0).astype(int)\n",
    "    \n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def normalize_per_100_poss(df):\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    df.iloc[:, 12:27] = 100*df.iloc[:, 12:27].div(df['PACE'], axis=0) \n",
    "    df.iloc[:,  34:-4] = 100*df.iloc[:, 34:-4].div(df['PACE'], axis=0) \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_matchups(df):\n",
    "    \"\"\"This function makes each row a matchup between \n",
    "    team and opp\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "\n",
    "    matchups = pd.merge(df, df.iloc[:, :-4], on=['GAME_ID'], suffixes=['', '_opp'])\n",
    "    matchups = matchups.loc[matchups['TEAM_ABBREVIATION'] != matchups['TEAM_ABBREVIATION_opp']]\n",
    "\n",
    "    matchups = matchups.drop(columns = ['SEASON_opp', 'TEAM_ABBREVIATION_opp', 'GAME_DATE_opp',\n",
    "                                        'MATCHUP_opp', 'HOME_GAME_opp', 'TEAM_NAME_opp', \n",
    "                                        'TEAM_ID_opp', 'WL_opp']\n",
    "                             )\n",
    "    \n",
    "    matchups\n",
    "    \n",
    "    return matchups\n",
    "\n",
    "\n",
    "def build_team_avg_stats_df(df: pd.DataFrame, span = 10) -> pd.DataFrame:    \n",
    "    \"\"\"This function finds the average for each team and opp statistic up to (and NOT including) the given date.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "\n",
    "    df = df.sort_values(['TEAM_ABBREVIATION', 'GAME_DATE']).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "\n",
    "    drop_cols = ['TEAM_ID', 'TEAM_NAME', 'GAME_ID', 'MATCHUP', \n",
    "                 'HOME_GAME', 'TEAM_SCORE', 'ML', 'SPREAD', \n",
    "                'GAME_DATE', 'POINT_DIFF', 'WL', 'TEAM_SCORE_opp',\n",
    "                'POINT_DIFF_opp', 'RECORD', 'RECORD_opp', 'TEAM_COVERED']\n",
    "\n",
    "    stats = df.drop(columns=drop_cols)\n",
    "\n",
    "    avg_stat_holder = []\n",
    "\n",
    "    for stat in stats.columns[2:]:\n",
    "        avg_stats = stats.groupby(['TEAM_ABBREVIATION'])[stat].ewm(span=span).mean().reset_index(drop=True)\n",
    "        avg_stat_holder.append(avg_stats)\n",
    "    \n",
    "    \n",
    "    matchup_info = df[['SEASON', 'TEAM_ABBREVIATION', 'GAME_DATE',\n",
    "                          'GAME_ID', 'MATCHUP', 'HOME_GAME', 'TEAM_SCORE',\n",
    "                          'ML', 'SPREAD', 'ATS_DIFF', 'RECORD', 'TEAM_COVERED', \n",
    "                          'POINT_DIFF', 'WL']]   \n",
    "\n",
    "    avg_stats = pd.concat(avg_stat_holder, axis=1)\n",
    "    \n",
    "\n",
    "    avg_stats = avg_stats.rename(columns={'ATS_DIFF':'AVG_ATS_DIFF'})\n",
    "    \n",
    "    avg_stats = pd.concat([matchup_info, avg_stats], axis=1)\n",
    "    \n",
    "    avg_stats['WIN_PCT'] = avg_stats.groupby(['TEAM_ABBREVIATION'])['RECORD'].rolling(window=span).mean().values\n",
    "    avg_stats['COVER_PCT'] = avg_stats.groupby(['TEAM_ABBREVIATION'])['TEAM_COVERED'].rolling(window=span).mean().values\n",
    "\n",
    "    avg_stats = avg_stats.drop(columns='RECORD')\n",
    "\n",
    "    avg_stats = avg_stats.sort_values(['TEAM_ABBREVIATION', 'GAME_DATE'])\n",
    "    avg_stats.iloc[:, 14:] = avg_stats.iloc[:, 14:].shift(1).where(avg_stats['TEAM_ABBREVIATION'].eq(avg_stats['TEAM_ABBREVIATION'].shift()))\n",
    "\n",
    "    avg_stats = avg_stats.add_suffix('_L{}'.format(span))\n",
    "    \n",
    "    avg_stats = avg_stats.rename(columns = {'SEASON_L{}'.format(span):'SEASON',\n",
    "                                           'TEAM_ABBREVIATION_L{}'.format(span):'TEAM_ABBREVIATION',\n",
    "                                           'GAME_DATE_L{}'.format(span):'GAME_DATE',\n",
    "                                           'GAME_ID_L{}'.format(span):'GAME_ID',\n",
    "                                           'MATCHUP_L{}'.format(span): 'MATCHUP', \n",
    "                                           'HOME_GAME_L{}'.format(span): 'HOME_GAME', \n",
    "                                           'TEAM_SCORE_L{}'.format(span):'TEAM_SCORE',\n",
    "                                           'ML_L{}'.format(span):'ML', \n",
    "                                           'SPREAD_L{}'.format(span):'SPREAD',\n",
    "                                           'ATS_DIFF_L{}'.format(span):'ATS_DIFF',\n",
    "                                           'RECORD_L{}'.format(span):'RECORD', \n",
    "                                           'TEAM_COVERED_L{}'.format(span):'TEAM_COVERED',\n",
    "                                           'POINT_DIFF_L{}'.format(span):'POINT_DIFF',\n",
    "                                           'WL_L{}'.format(span):'WL'})\n",
    "    \n",
    "    return avg_stats\n",
    "\n",
    "\n",
    "def add_percentage_features(df, span):\n",
    "    \"\"\"Add the following features for both team and opp:\n",
    "    OREB_PCT, DREB_PCT, REB_PCT, TS_PCT, EFG_PCT, AST_RATIO, TOV_PCT, PIE.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    df['OREB_PCT_L{}'.format(span)] = df['OREB_L{}'.format(span)] / (df['OREB_L{}'.format(span)] + df['DREB_opp_L{}'.format(span)])\n",
    "    df['OREB_PCT_opp_L{}'.format(span)] = df['OREB_opp_L{}'.format(span)] / (df['OREB_opp_L{}'.format(span)] + df['DREB_L{}'.format(span)])\n",
    "\n",
    "    df['DREB_PCT_L{}'.format(span)] = df['DREB_L{}'.format(span)] / (df['DREB_L{}'.format(span)] + df['OREB_opp_L{}'.format(span)])\n",
    "    df['DREB_PCT_opp_L{}'.format(span)] = df['DREB_opp_L{}'.format(span)] / (df['DREB_opp_L{}'.format(span)] + df['OREB_L{}'.format(span)])\n",
    "\n",
    "    df['REB_PCT_L{}'.format(span)] = df['REB_L{}'.format(span)] / (df['REB_L{}'.format(span)] + df['REB_opp_L{}'.format(span)])\n",
    "    df['REB_PCT_opp_L{}'.format(span)] = df['REB_opp_L{}'.format(span)] / (df['REB_opp_L{}'.format(span)] + df['REB_L{}'.format(span)])\n",
    "\n",
    "    df['TS_PCT_L{}'.format(span)] = df['PTS_L{}'.format(span)] / ((2*(df['FG2A_L{}'.format(span)] + df['FG3A_L{}'.format(span)]) + 0.44*df['FTA_L{}'.format(span)]))\n",
    "    \n",
    "    df['TS_PCT_opp_L{}'.format(span)] = df['PTS_opp_L{}'.format(span)] / ((2*(df['FG2A_opp_L{}'.format(span)] + df['FG3A_opp_L{}'.format(span)]) + 0.44*df['FTA_opp_L{}'.format(span)]))\n",
    "\n",
    "    df['EFG_PCT_L{}'.format(span)] = (df['FG2M_L{}'.format(span)] + 1.5*df['FG3M_L{}'.format(span)]) / (df['FG2A_L{}'.format(span)]\n",
    "                                                                    + df['FG3A_L{}'.format(span)])\n",
    "    df['EFG_PCT_opp_L{}'.format(span)] = (df['FG2M_opp_L{}'.format(span)] + 1.5*df['FG3M_opp_L{}'.format(span)]) / (df['FG2A_opp_L{}'.format(span)] \n",
    "                                                                 + df['FG3A_opp_L{}'.format(span)])\n",
    "\n",
    "    df['AST_RATIO_L{}'.format(span)] = (df['AST_L{}'.format(span)] * 100) / df['PACE_L{}'.format(span)]\n",
    "    df['AST_RATIO_opp_L{}'.format(span)] = (df['AST_opp_L{}'.format(span)] * 100) / df['PACE_opp_L{}'.format(span)]\n",
    "\n",
    "    df['TOV_PCT_L{}'.format(span)] = 100*df['TOV_L{}'.format(span)] / (df['FG2A_L{}'.format(span)] \n",
    "                                               + df['FG3A_L{}'.format(span)] \n",
    "                                               + 0.44*df['FTA_L{}'.format(span)] \n",
    "                                               + df['TOV_L{}'.format(span)])\n",
    "    \n",
    "    df['TOV_PCT_opp_L{}'.format(span)] = 100*df['TOV_opp_L{}'.format(span)] / (df['FG2A_opp_L{}'.format(span)] \n",
    "                                             + df['FG3A_opp_L{}'.format(span)] \n",
    "                                             + 0.44*df['FTA_opp_L{}'.format(span)] \n",
    "                                             + df['TOV_opp_L{}'.format(span)])\n",
    "    \n",
    "    \n",
    "    df['PIE_L{}'.format(span)] = ((df['PTS_L{}'.format(span)] + df['FG2M_L{}'.format(span)] + df['FG3M_L{}'.format(span)] + df['FTM_L{}'.format(span)] \n",
    "                 - df['FG2A_L{}'.format(span)] - df['FG3A_L{}'.format(span)] - df['FTA_L{}'.format(span)] \n",
    "                 + df['DREB_L{}'.format(span)] + df['OREB_L{}'.format(span)]/2\n",
    "                + df['AST_L{}'.format(span)] + df['STL_L{}'.format(span)] + df['BLK_L{}'.format(span)]/2\n",
    "                - df['PF_L{}'.format(span)] - df['TOV_L{}'.format(span)]) \n",
    "                 / (df['PTS_L{}'.format(span)] + df['PTS_opp_L{}'.format(span)] + df['FG2M_L{}'.format(span)] + df['FG2M_opp_L{}'.format(span)]\n",
    "                   + df['FG3M_L{}'.format(span)] + df['FG3M_opp_L{}'.format(span)] + df['FTM_L{}'.format(span)] + df['FTM_opp_L{}'.format(span)]\n",
    "                   - df['FG2A_L{}'.format(span)] - df['FG2A_opp_L{}'.format(span)] - df['FG3A_L{}'.format(span)] - df['FG3A_opp_L{}'.format(span)] \n",
    "                    - df['FTA_L{}'.format(span)] - df['FTA_opp_L{}'.format(span)] + df['DREB_L{}'.format(span)] + df['DREB_opp_L{}'.format(span)]\n",
    "                    + (df['OREB_L{}'.format(span)]+df['OREB_opp_L{}'.format(span)])/2 + df['AST_L{}'.format(span)] + df['AST_opp_L{}'.format(span)]\n",
    "                    + df['STL_L{}'.format(span)] + df['STL_opp_L{}'.format(span)] + (df['BLK_L{}'.format(span)] + df['BLK_opp_L{}'.format(span)])/2\n",
    "                    - df['PF_L{}'.format(span)] - df['PF_opp_L{}'.format(span)] - df['TOV_L{}'.format(span)] - df['TOV_opp_L{}'.format(span)]))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rest_days(df):\n",
    "    \n",
    "    df['prev_game'] = df.groupby(['SEASON', 'TEAM_ABBREVIATION'])['GAME_DATE'].shift(1)\n",
    "\n",
    "    df['REST'] = (df['GAME_DATE'] - df['prev_game']) / np.timedelta64(1, 'D')\n",
    "            \n",
    "    df.loc[df['REST'] >= 8, 'REST'] = 8\n",
    "    \n",
    "    df = df.drop(columns=['prev_game'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def season_to_string(x):\n",
    "    return str(x) + '-' + str(x+1)[-2:]\n",
    "\n",
    "\n",
    "def load_and_process_data(start_season, end_season):\n",
    "    start_season = season_to_string(start_season)\n",
    "    end_season = season_to_string(end_season)\n",
    "\n",
    "    db_filepath = Path.home().joinpath('NBA_model_v1', 'data', 'nba.db')\n",
    "\n",
    "    conn = sqlite3.connect(db_filepath)\n",
    "    \n",
    "    print(\"Loading raw team boxscore data from sql database...\")\n",
    "    \n",
    "    df = load_team_data(conn, start_season, end_season)\n",
    "    print(\"Loading betting data from sql database...\")\n",
    "    spreads, moneylines = load_betting_data(conn)\n",
    "    \n",
    "    print(\"Cleaning Data...\")\n",
    "    df = clean_team_data(df)\n",
    "    df = prep_for_aggregation(df)\n",
    "\n",
    "    clean_mls = clean_moneyline_df(df = moneylines)\n",
    "    clean_spreads = clean_spreads_df(df = spreads)\n",
    "    \n",
    "    print(\"Merging Boxscore and Betting Data...\")\n",
    "    merged_df = merge_betting_and_boxscore_data(\n",
    "        clean_spreads, clean_mls, clean_boxscores = df)\n",
    "    \n",
    "    \n",
    "    stats_per_100 = normalize_per_100_poss(merged_df)\n",
    "\n",
    "    print(\"Aggregating over last 5, 10, and 20 game windows\")\n",
    "    \n",
    "    matchups = create_matchups(stats_per_100)\n",
    "    \n",
    "    team_stats_ewa_5 = build_team_avg_stats_df(matchups, span=5)\n",
    "    team_stats_ewa_5 = add_percentage_features(team_stats_ewa_5, span=5)\n",
    "\n",
    "    team_stats_ewa_10 = build_team_avg_stats_df(matchups, span=10)\n",
    "    team_stats_ewa_10 = add_percentage_features(team_stats_ewa_10, span=10)\n",
    "\n",
    "    team_stats_ewa_20 = build_team_avg_stats_df(matchups, span=20)\n",
    "    team_stats_ewa_20 = add_percentage_features(team_stats_ewa_20, span=20)\n",
    "\n",
    "\n",
    "    temp = pd.merge(team_stats_ewa_5, team_stats_ewa_10, how='inner',\n",
    "                    on=['SEASON', 'TEAM_ABBREVIATION', 'GAME_DATE',\n",
    "                        'GAME_ID', 'MATCHUP', 'HOME_GAME', 'TEAM_SCORE',\n",
    "                        'ML', 'SPREAD', 'ATS_DIFF', 'TEAM_COVERED', \n",
    "                        'POINT_DIFF', 'WL'])\n",
    "\n",
    "    df_full = pd.merge(temp, team_stats_ewa_20, how='inner', \n",
    "                       on=['SEASON', 'TEAM_ABBREVIATION', 'GAME_DATE',\n",
    "                            'GAME_ID', 'MATCHUP', 'HOME_GAME', 'TEAM_SCORE',\n",
    "                            'ML', 'SPREAD', 'ATS_DIFF', 'TEAM_COVERED', \n",
    "                            'POINT_DIFF', 'WL'])\n",
    "\n",
    "    df_full = df_full.sort_values(['GAME_DATE', 'GAME_ID', 'HOME_GAME'])\n",
    "    \n",
    "    \n",
    "    columns_to_drop = ['PTS_L5', 'PTS_L10', 'PTS_L20',\n",
    "                        'PLUS_MINUS_L5', 'PLUS_MINUS_L10', 'PLUS_MINUS_L20',\n",
    "                        'NET_RATING_L5', 'NET_RATING_L10', 'NET_RATING_L20',\n",
    "                        'POSS_L5', 'POSS_L10', 'POSS_L20',\n",
    "                        'REB_L5', 'REB_L10', 'REB_L20',\n",
    "                        'REB_opp_L5', 'REB_opp_L10', 'REB_opp_L20',\n",
    "                        'PTS_opp_L5', 'PTS_opp_L10', 'PTS_opp_L20',\n",
    "                        'PLUS_MINUS_opp_L5', 'PLUS_MINUS_opp_L10', 'PLUS_MINUS_opp_L20',\n",
    "                        'NET_RATING_opp_L5', 'NET_RATING_opp_L10', 'NET_RATING_opp_L20',\n",
    "                        'POSS_opp_L5', 'POSS_opp_L10', 'POSS_opp_L20']\n",
    "    \n",
    "    df_full = df_full.drop(columns = columns_to_drop)\n",
    "    \n",
    "    print(\"adding rest days\")\n",
    "    df_full = add_rest_days(df_full)\n",
    "    \n",
    "    return df_full\n",
    "    \n",
    "\n",
    "def make_matchup_row(home_team, away_team, df):\n",
    "    \n",
    "    print(\"creating matchups between Home and Away team aggregated stats\")\n",
    "\n",
    "    matchup_info_cols = ['SEASON', 'TEAM_ABBREVIATION', 'GAME_DATE', 'GAME_ID', 'MATCHUP',\n",
    "        'HOME_GAME', 'TEAM_SCORE', 'ML', 'SPREAD', 'ATS_DIFF', 'TEAM_COVERED',\n",
    "        'POINT_DIFF', 'WL']\n",
    "\n",
    "    most_recent_home_stats = df.loc[df['TEAM_ABBREVIATION'] == home_team].tail(1).drop(columns=matchup_info_cols).values\n",
    "    most_recent_away_stats = df.loc[df['TEAM_ABBREVIATION'] == away_team].tail(1).drop(columns=matchup_info_cols).values\n",
    "\n",
    "    matchup_row = pd.DataFrame(np.concatenate([most_recent_home_stats, most_recent_away_stats], axis=1), columns=X_train.columns)\n",
    "        \n",
    "    return matchup_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw team boxscore data from sql database...\n",
      "Loading betting data from sql database...\n",
      "Cleaning Data...\n",
      "Merging Boxscore and Betting Data...\n",
      "Aggregating over last 5, 10, and 20 game windows\n",
      "adding rest days\n"
     ]
    }
   ],
   "source": [
    "df_full = load_and_process_data(start_season = 2013, end_season = 2021)\n",
    "\n",
    "\n",
    "# row = get_data_for_matchup(home_team='LAL', away_team='OKC', start_season=2013, end_season=2021, table_name='team_stats_ewa_matchup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating matchups between Home and Away team aggregated stats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[110.81939504, 111.24257733]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_team = 'GSW'\n",
    "away_team = 'LAL'\n",
    "\n",
    "row = make_matchup_row(home_team, away_team, df = df_full)\n",
    "lgbr_model.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating matchups between Home and Away team aggregated stats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[102.70060833, 100.81850387]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_team = 'PHI'\n",
    "away_team = 'BOS'\n",
    "\n",
    "row = make_matchup_row(home_team, away_team, df = df_full)\n",
    "lgbr_model.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating matchups between Home and Away team aggregated stats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[113.77664723, 114.94091434]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_team = 'LAL'\n",
    "away_team = 'OKC'\n",
    "\n",
    "row = make_matchup_row(home_team, away_team, df = df_full)\n",
    "lgbr_model.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[113.77664723, 114.94091434]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbr_model.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw team boxscore data from sql database...\n",
      "Loading betting data from sql database...\n",
      "Cleaning Data...\n",
      "Merging Boxscore and Betting Data...\n",
      "Aggregating over last 5, 10, and 20 game windows\n",
      "adding rest days\n"
     ]
    }
   ],
   "source": [
    "home_team = 'LAL'\n",
    "away_team = 'SAS'\n",
    "start_season = 2013\n",
    "end_season = 2021\n",
    "\n",
    "start_season = season_to_string(start_season)\n",
    "end_season = season_to_string(end_season)\n",
    "\n",
    "db_filepath = Path.home().joinpath('NBA_model_v1', 'data', 'nba.db')\n",
    "\n",
    "conn = sqlite3.connect(db_filepath)\n",
    "\n",
    "print(\"Loading raw team boxscore data from sql database...\")\n",
    "\n",
    "df = load_team_data(conn, start_season, end_season)\n",
    "print(\"Loading betting data from sql database...\")\n",
    "spreads, moneylines = load_betting_data(conn)\n",
    "\n",
    "print(\"Cleaning Data...\")\n",
    "\n",
    "df = clean_team_data(df)\n",
    "df = prep_for_aggregation(df)\n",
    "\n",
    "clean_mls = clean_moneyline_df(df = moneylines)\n",
    "clean_spreads = clean_spreads_df(df = spreads)\n",
    "\n",
    "\n",
    "print(\"Merging Boxscore and Betting Data...\")\n",
    "merged_df = merge_betting_and_boxscore_data(\n",
    "    clean_spreads, clean_mls, clean_boxscores = df)\n",
    "\n",
    "\n",
    "stats_per_100 = normalize_per_100_poss(merged_df)\n",
    "\n",
    "print(\"Aggregating over last 5, 10, and 20 game windows\")\n",
    "\n",
    "matchups = create_matchups(stats_per_100)\n",
    "\n",
    "team_stats_ewa_5 = build_team_avg_stats_df(matchups, span=5)\n",
    "team_stats_ewa_5 = add_percentage_features(team_stats_ewa_5, span=5)\n",
    "\n",
    "team_stats_ewa_10 = build_team_avg_stats_df(matchups, span=10)\n",
    "team_stats_ewa_10 = add_percentage_features(team_stats_ewa_10, span=10)\n",
    "\n",
    "team_stats_ewa_20 = build_team_avg_stats_df(matchups, span=20)\n",
    "team_stats_ewa_20 = add_percentage_features(team_stats_ewa_20, span=20)\n",
    "\n",
    "\n",
    "temp = pd.merge(team_stats_ewa_5, team_stats_ewa_10, how='inner',\n",
    "                on=['SEASON', 'TEAM_ABBREVIATION', 'GAME_DATE',\n",
    "                    'GAME_ID', 'MATCHUP', 'HOME_GAME', 'TEAM_SCORE',\n",
    "                    'ML', 'SPREAD', 'ATS_DIFF', 'TEAM_COVERED', \n",
    "                    'POINT_DIFF', 'WL'])\n",
    "\n",
    "df_full = pd.merge(temp, team_stats_ewa_20, how='inner', \n",
    "                    on=['SEASON', 'TEAM_ABBREVIATION', 'GAME_DATE',\n",
    "                        'GAME_ID', 'MATCHUP', 'HOME_GAME', 'TEAM_SCORE',\n",
    "                        'ML', 'SPREAD', 'ATS_DIFF', 'TEAM_COVERED', \n",
    "                        'POINT_DIFF', 'WL'])\n",
    "\n",
    "df_full = df_full.sort_values(['GAME_DATE', 'GAME_ID', 'HOME_GAME'])\n",
    "\n",
    "\n",
    "columns_to_drop = ['PTS_L5', 'PTS_L10', 'PTS_L20',\n",
    "                    'PLUS_MINUS_L5', 'PLUS_MINUS_L10', 'PLUS_MINUS_L20',\n",
    "                    'NET_RATING_L5', 'NET_RATING_L10', 'NET_RATING_L20',\n",
    "                    'POSS_L5', 'POSS_L10', 'POSS_L20',\n",
    "                    'REB_L5', 'REB_L10', 'REB_L20',\n",
    "                    'REB_opp_L5', 'REB_opp_L10', 'REB_opp_L20',\n",
    "                    'PTS_opp_L5', 'PTS_opp_L10', 'PTS_opp_L20',\n",
    "                    'PLUS_MINUS_opp_L5', 'PLUS_MINUS_opp_L10', 'PLUS_MINUS_opp_L20',\n",
    "                    'NET_RATING_opp_L5', 'NET_RATING_opp_L10', 'NET_RATING_opp_L20',\n",
    "                    'POSS_opp_L5', 'POSS_opp_L10', 'POSS_opp_L20']\n",
    "\n",
    "\n",
    "df_full = df_full.drop(columns = columns_to_drop)\n",
    "\n",
    "print(\"adding rest days\")\n",
    "df_full = add_rest_days(df_full)\n",
    "\n",
    "print(\"creating matchups between Home and Away team aggregated stats\")\n",
    "\n",
    "matchup_info_cols = ['SEASON', 'TEAM_ABBREVIATION', 'GAME_DATE', 'GAME_ID', 'MATCHUP',\n",
    "       'HOME_GAME', 'TEAM_SCORE', 'ML', 'SPREAD', 'ATS_DIFF', 'TEAM_COVERED',\n",
    "       'POINT_DIFF', 'WL']\n",
    "\n",
    "most_recent_home_stats = df_full.loc[df_full['TEAM_ABBREVIATION'] == home_team].tail(1).drop(columns=matchup_info_cols).values\n",
    "most_recent_away_stats = df_full.loc[df_full['TEAM_ABBREVIATION'] == away_team].tail(1).drop(columns=matchup_info_cols).values\n",
    "\n",
    "matchup_row = pd.DataFrame(np.concatenate([most_recent_home_stats, most_recent_away_stats], axis=1), columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating matchups between Home and Away team aggregated stats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG2M_L5</th>\n",
       "      <th>FG2A_L5</th>\n",
       "      <th>FG3M_L5</th>\n",
       "      <th>FG3A_L5</th>\n",
       "      <th>FTM_L5</th>\n",
       "      <th>FTA_L5</th>\n",
       "      <th>OREB_L5</th>\n",
       "      <th>DREB_L5</th>\n",
       "      <th>AST_L5</th>\n",
       "      <th>STL_L5</th>\n",
       "      <th>...</th>\n",
       "      <th>TS_PCT_L20</th>\n",
       "      <th>TS_PCT_opp_L20</th>\n",
       "      <th>EFG_PCT_L20</th>\n",
       "      <th>EFG_PCT_opp_L20</th>\n",
       "      <th>AST_RATIO_L20</th>\n",
       "      <th>AST_RATIO_opp_L20</th>\n",
       "      <th>TOV_PCT_L20</th>\n",
       "      <th>TOV_PCT_opp_L20</th>\n",
       "      <th>PIE_L20</th>\n",
       "      <th>REST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10810</th>\n",
       "      <td>29.598352</td>\n",
       "      <td>53.032355</td>\n",
       "      <td>8.836455</td>\n",
       "      <td>29.109184</td>\n",
       "      <td>23.71382</td>\n",
       "      <td>31.905133</td>\n",
       "      <td>7.354883</td>\n",
       "      <td>34.745477</td>\n",
       "      <td>21.104468</td>\n",
       "      <td>7.071961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614004</td>\n",
       "      <td>0.630166</td>\n",
       "      <td>0.54171</td>\n",
       "      <td>0.569435</td>\n",
       "      <td>23.057299</td>\n",
       "      <td>27.382987</td>\n",
       "      <td>12.22665</td>\n",
       "      <td>11.003484</td>\n",
       "      <td>0.461889</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 283 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FG2M_L5    FG2A_L5   FG3M_L5    FG3A_L5    FTM_L5     FTA_L5  \\\n",
       "10810  29.598352  53.032355  8.836455  29.109184  23.71382  31.905133   \n",
       "\n",
       "        OREB_L5    DREB_L5     AST_L5    STL_L5  ...  TS_PCT_L20  \\\n",
       "10810  7.354883  34.745477  21.104468  7.071961  ...    0.614004   \n",
       "\n",
       "       TS_PCT_opp_L20  EFG_PCT_L20  EFG_PCT_opp_L20  AST_RATIO_L20  \\\n",
       "10810        0.630166      0.54171         0.569435      23.057299   \n",
       "\n",
       "       AST_RATIO_opp_L20  TOV_PCT_L20  TOV_PCT_opp_L20   PIE_L20  REST  \n",
       "10810          27.382987     12.22665        11.003484  0.461889   2.0  \n",
       "\n",
       "[1 rows x 283 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AWAY_FG2M_L5</th>\n",
       "      <th>AWAY_FG2A_L5</th>\n",
       "      <th>AWAY_FG3M_L5</th>\n",
       "      <th>AWAY_FG3A_L5</th>\n",
       "      <th>AWAY_FTM_L5</th>\n",
       "      <th>AWAY_FTA_L5</th>\n",
       "      <th>AWAY_OREB_L5</th>\n",
       "      <th>AWAY_DREB_L5</th>\n",
       "      <th>AWAY_AST_L5</th>\n",
       "      <th>AWAY_STL_L5</th>\n",
       "      <th>...</th>\n",
       "      <th>AWAY_TS_PCT_L20</th>\n",
       "      <th>AWAY_TS_PCT_opp_L20</th>\n",
       "      <th>AWAY_EFG_PCT_L20</th>\n",
       "      <th>AWAY_EFG_PCT_opp_L20</th>\n",
       "      <th>AWAY_AST_RATIO_L20</th>\n",
       "      <th>AWAY_AST_RATIO_opp_L20</th>\n",
       "      <th>AWAY_TOV_PCT_L20</th>\n",
       "      <th>AWAY_TOV_PCT_opp_L20</th>\n",
       "      <th>AWAY_PIE_L20</th>\n",
       "      <th>AWAY_REST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20663</th>\n",
       "      <td>29.125098</td>\n",
       "      <td>53.691494</td>\n",
       "      <td>11.348029</td>\n",
       "      <td>31.593497</td>\n",
       "      <td>20.697453</td>\n",
       "      <td>27.793212</td>\n",
       "      <td>6.999337</td>\n",
       "      <td>37.758586</td>\n",
       "      <td>25.913601</td>\n",
       "      <td>6.192778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601159</td>\n",
       "      <td>0.583452</td>\n",
       "      <td>0.533056</td>\n",
       "      <td>0.519636</td>\n",
       "      <td>27.158502</td>\n",
       "      <td>23.635871</td>\n",
       "      <td>11.279083</td>\n",
       "      <td>11.404486</td>\n",
       "      <td>0.525536</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 283 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AWAY_FG2M_L5  AWAY_FG2A_L5  AWAY_FG3M_L5  AWAY_FG3A_L5  AWAY_FTM_L5  \\\n",
       "20663     29.125098     53.691494     11.348029     31.593497    20.697453   \n",
       "\n",
       "       AWAY_FTA_L5  AWAY_OREB_L5  AWAY_DREB_L5  AWAY_AST_L5  AWAY_STL_L5  ...  \\\n",
       "20663    27.793212      6.999337     37.758586    25.913601     6.192778  ...   \n",
       "\n",
       "       AWAY_TS_PCT_L20  AWAY_TS_PCT_opp_L20  AWAY_EFG_PCT_L20  \\\n",
       "20663         0.601159             0.583452          0.533056   \n",
       "\n",
       "       AWAY_EFG_PCT_opp_L20  AWAY_AST_RATIO_L20  AWAY_AST_RATIO_opp_L20  \\\n",
       "20663              0.519636           27.158502               23.635871   \n",
       "\n",
       "       AWAY_TOV_PCT_L20  AWAY_TOV_PCT_opp_L20  AWAY_PIE_L20  AWAY_REST  \n",
       "20663         11.279083             11.404486      0.525536        1.0  \n",
       "\n",
       "[1 rows x 283 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchup_info_cols = ['SEASON', 'TEAM_ABBREVIATION', 'GAME_DATE', 'GAME_ID', 'MATCHUP',\n",
    "       'HOME_GAME', 'TEAM_SCORE', 'ML', 'SPREAD', 'ATS_DIFF', 'TEAM_COVERED',\n",
    "       'POINT_DIFF', 'WL']\n",
    "most_recent_home_stats = df_full.loc[df_full['TEAM_ABBREVIATION'] == home_team].tail(1).drop(columns=matchup_info_cols)\n",
    "most_recent_away_stats = df_full.loc[df_full['TEAM_ABBREVIATION'] == away_team].tail(1).drop(columns=matchup_info_cols)\n",
    "\n",
    "most_recent_away_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022-10-18'\n",
    "\n",
    "web = 'https://www.sportsbookreview.com/betting-odds/nba-basketball/?date={}'.format(date)\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(web)\n",
    "sleep(random.randint(2,3))\n",
    "\n",
    "# try:\n",
    "#     single_row_events = driver.find_elements(By.CLASS_NAME, 'eventMarketGridContainer-3QipG')\n",
    "    \n",
    "# except:\n",
    "#     print(\"No Data for {}\".format(date))\n",
    "#     dates_with_no_data.append(date)\n",
    "#     continue\n",
    "    \n",
    "# num_postponed_events = len(driver.find_elements(By.CLASS_NAME, 'eventStatus-3EHqw'))\n",
    "\n",
    "# num_listed_events = len(single_row_events)\n",
    "# cutoff = num_listed_events - num_postponed_events\n",
    "\n",
    "# for event in single_row_events[:cutoff]:\n",
    "\n",
    "#     away_team = event.find_elements(By.CLASS_NAME, 'participantBox-3ar9Y')[0].text\n",
    "#     home_team = event.find_elements(By.CLASS_NAME, 'participantBox-3ar9Y')[1].text\n",
    "#     away_teams.append(away_team)\n",
    "#     home_teams.append(home_team)\n",
    "#     gm_dates.append(date)\n",
    "\n",
    "#     seasons.append(season_string(season))\n",
    "    \n",
    "#     scoreboard = event.find_elements(By.CLASS_NAME, 'scoreboard-1TXQV')\n",
    "\n",
    "#     home_score = []\n",
    "#     away_score = []\n",
    "\n",
    "#     for score in scoreboard:\n",
    "#         quarters = score.find_elements(By.CLASS_NAME, 'scoreboardColumn-2OtpR')\n",
    "#         for i in range(len(quarters)):\n",
    "#             scores = quarters[i].text.split('\\n')\n",
    "#             away_score.append(scores[0])\n",
    "#             home_score.append(scores[1])\n",
    "            \n",
    "#         home_score = \",\".join(home_score)\n",
    "#         away_score = \",\".join(away_score)\n",
    "        \n",
    "#         away_scoreboards.append(away_score)\n",
    "#         home_scoreboards.append(home_score)\n",
    "\n",
    "\n",
    "#     if len(away_scoreboards) != len(away_teams):\n",
    "#         num_to_add = len(away_teams) - len(away_scoreboards)\n",
    "#         for i in range(num_to_add):\n",
    "#             away_scoreboards.append('')\n",
    "#             home_scoreboards.append('')\n",
    "\n",
    "#     spreads = event.find_elements(By.CLASS_NAME, 'pointer-2j4Dk')\n",
    "#     away_lines = []\n",
    "#     home_lines = []\n",
    "#     for i in range(len(spreads)):    \n",
    "#         if i % 2 == 0:\n",
    "#             away_lines.append(spreads[i].text)\n",
    "#         else:\n",
    "#             home_lines.append(spreads[i].text)\n",
    "    \n",
    "#     away_lines = \",\".join(away_lines)\n",
    "#     home_lines = \",\".join(home_lines)\n",
    "    \n",
    "#     away_spreads.append(away_lines)\n",
    "#     home_spreads.append(home_lines)\n",
    "\n",
    "#     if len(away_spreads) != len(away_teams):\n",
    "#         num_to_add = len(away_teams) - len(away_spreads)\n",
    "#         for i in range(num_to_add):\n",
    "#             away_scoreboards.append('')\n",
    "#             home_scoreboards.append('')\n",
    "\n",
    "# driver.quit()\n",
    "# clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nba-model-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afed4e3e0de72dfe896065169312871c2edc1b77c10ac917e4bc0d2d23ff8bb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
